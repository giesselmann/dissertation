\chapter{Nanopype Processing Pipeline}
\label{cha:nanopype}

Long-read third-generation nanopore sequencing enables researchers to now address a range of questions such as genome assembly, structural variant and isoform detection that are difficult to tackle with short read approaches. The rapidly expanding user base and continuously increasing throughput have sparked the development of a growing number of specialized analysis tools. However, streamlined processing of nanopore datasets using reproducible and transparent workflows is still lacking. Therefore we developed \textit{Nanopype}, a nanopore data processing pipeline that integrates a diverse set of established bioinformatics software while maintaining consistent and standardized output formats. Seamless integration into compute cluster environments makes the framework suitable for high-throughput applications. As a result, \textit{Nanopype} facilitates the comparability of nanopore data analysis workflows and thereby should enhance the reproducibility of biological insights. \textit{Nanopype} is available at \textit{https://github.com/giesselmann/nanopype}.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/nanopype/GA.pdf}
    \label{fig:nanopype:ga}
\end{figure}

\textbf{Note:} This chapter is based on the publication P. Giesselmann et al. \textit{Nanopype: a modular and scalable nanopore data processing pipeline}, Bioinformatics, 2019 and contains text from the original paper.

The chapter starts with a brief \textbf{background} in \ref{sec:nanopype:background} followed by high-level pipeline \textbf{design} decisions covering storage, tool encapsulation and reproducibility in section \ref{sec:nanopype:design}. Grouped into \textbf{modules} individual tools are highlighted in section \ref{sec:nanopype:modules}.
The \textbf{installation} section \ref{sec:nanopype:installation} illustrates the setup and configuration of the pipeline in different environments. 
Finally the \textbf{usage} on a daily production level is outlined in section \ref{sec:nanopype:usage}.




\section{Background}
\label{sec:nanopype:background}

Due to constant development and improvement of applications, frequent reprocessing of the raw signal and downstream data is necessary. Based on the same chemistry and pore version, the single read accuracy could be enhanced from 90\% to 95\% modal in software over the past years. Thus, novel archiving and processing strategies are needed for data storage and handling that scale with the large amount of data produced by the MinION sequencer. Even when using the most recent compression, the raw signal data is typically five times larger than the sequenced base pairs, resulting in hundreds of gigabytes per sequencing run. This will still be more relevant as higher throughput devices such as the PromethION become more widely available. Furthermore, a limiting factor of the applicability of this new technology are the currently available, research-grade software packages for nanopore long read data analyses. These tend to be difficult to install and require complex software environments. Despite the growing number of recently developed algorithms \cite{Magi2018}, primary data processing remains challenging due to stand-alone tools without congruent data formats and requirements. 
Most recent examples of nanopore data processing pipelines are \textit{Katuali}\footnote{github.com/nanoporetech/katuali} for basecalling and assembly and \textit{Pinfish}\footnote{github.com/nanoporetech/pipeline-pinfish-analysis} for RNA isoform detection from cDNA and direct RNA sequencing experiments. However, they have been developed to perform very specific and inflexible analysis workflows without integrated handling of the critical raw data storage or version control of wrapped tools.

To overcome these issues, we have developed \textit{Nanopype}, a pipeline designed explicitly for streamlined and automated nanopore long read processing. Apart from the integration of essential basecalling, quality control, and alignment tools, we facilitate a set of publicly available analysis applications for barcode demultiplexing, DNA methylation readout, structural variant calling, RNA isoform detection and genome assembly. 
Based on the \textit{Snakemake} engine \cite{Koester2012}, our method integrates established error handling and uniform output structures across multiple experiments. Furthermore, \textit{Nanopype} can be run in a parallel setup on both single computers and server clusters. Deployed as a python module, \textit{Nanopype} is mostly built from source with encapsulated routines to simplify the initial setup and integration into existing environments. Additionally, we provide Singularity images for all modules and an automatically built all-in-one Docker container. This enables the usage of the pipeline for both less bioinformatically experienced experimental scientists and bioinformaticians. Lastly, Nanopype provides a well-defined framework for standardized processing independent of the underlying operating system.




\section{Design}
\label{sec:nanopype:design}
\textit{Nanopype’s} core element is a modular setup to easily update existing tools and to allow seamless integration of the latest developments. Nonetheless, each pipeline release is freezing the included tool versions to guarantee reproducible results. \textit{Snakemake} as workflow management is chosen over the competitor \textit{Nextflow} to support a custom cluster engine and preferring Python over Java for rapid development. In a nutshell, \textit{Nanopype} has been designed around three key components: raw data storage, tool encapsulation and standardized directory structures that mirror the applied toolchain.

\begin{tcolorbox}[breakable,title=Snakemake]
	\textit{Snakemake} is a Python based workflow management system. Reproducibility and scaling on cluster environments make it preferable over plain bash script or Makefile based pipelines. As a core concept, \textit{Snakemake} is built around \textit{rules} defining steps to compute an output from an input file. By requesting a single output file, the user initiates the construction of a directed acyclic graph (DAG), chaining all intermediate steps needed to complete the workflow. With the inputs of sequences and reference genome, an alignment rule could for instance compute alignments into an intermediate SAM format to be further sorted and converted to BAM-files. \textit{Snakemake} workflows are defined through the resulting file and folder structure and require a shared file system across compute nodes.
\end{tcolorbox}


\subsection{Storage}
\label{subsec:nanopype:storage}
The first core design component is the consistent storage of the raw signal data from any ONT sequencer. 
Raw nanopore reads are stored in FAST5 files, an ONT specification of the universal HDF5 file format. Initially, the sequencers exported one file per read, resulting in hundreds of thousands of files per sequencing run. 
The number of files generated could, especially on Linux file systems, disrupt background services such as nightly mirrors and backups.
\textit{Nanopype} is backward compatible with datasets of single read FAST5 files, for which we provide a module to import and package single reads into TAR archive batches. More recently, in 2019 ONT utilized the full functionality of the HDF5 format with groups (comparable to directories), datasets (structured arrays of primitive data types) and attributes (single values for meta information) and released the multi-read-FAST5 format. The new format typically groups 4000 reads into a single file, resulting in notable improvements regarding copy and synchronization tasks. Lately, the replacement of the \textit{GZIP} by the custom \textit{VBZ}\footnote{https://github.com/nanoporetech/vbz\_compression} compression reduced file sizes by approximately 30\%.

Besides the \textit{Nanopype} pipeline we propose a multi-device and multi-user nanopore sequencing setup (Fig. \ref{fig:nanopype:storage}). Provided with a suitable server infrastructure, the design supports processing of multiple sequencing runs per week from both local and remotely connected devices. 
Deploying \textit{syncthing}\footnote{https://github.com/syncthing/syncthing} on device and server side, we synchronize the output folder from the ONT sequencing software \textit{MinKNOW} with device specific folders in a central spooling area.
After completion of the sequencing, the data is moved to the active storage, the file ownership is changed to a particular \textit{data user} and files are made write-protected (Fig. \ref{fig:nanopype:storage}).

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/nanopype/storage.pdf}
	\captionsetup{format=plain}
	\caption[Nanopype data flow]{Data flow for multi-sequencer and multi-user setup: Raw data generated on local or remote devices is synchronized into a local spooling directory. Complete runs with meta information (PAD75042: flow cell ID, HUES64: sample name) are stored with read-only access and archived on tape. Processing combines multiple sequencing runs of the same sample and provides basic readout for downstream analysis.}
	\label{fig:nanopype:storage}
\end{figure}

The active storage contains one folder per sequencing run following a uniform naming pattern of date, flow cell ID, flow cell type, sequencing kit and a sequence of arbitrary user tags e.g.:

\begin{center}
	20200401\_FAH12345\_FLO-MIN106\_SQK-LSK109\_HUES64\_WT
\end{center}

From the active storage, sequencing runs can be archived on e.g. tape drives for long term data retention. For the processing, sequencing runs are mapped into an interface (a directory with softlinks into the active storage). This additional layer allows the distribution of raw data across multiple physical devices, while maintaining consistent access.
The raw data archive forms the basis for any downstream analyses and enables smooth re-processing of legacy datasets as soon as for example improved basecalling algorithms become available.


\subsection{Encapsulation}
\label{subsec:nanopype:encapsulation}
The installation of experimental software packages still being actively developed with complex library dependencies can be time-consuming, but remains essential to make use of the current-generation nanopore analysis workflows. On a base level, \textit{Nanopype} uses \textit{Snakemake} rules to wrap the build from source and installation process of its dependencies and therefore does not require root privileges for the setup on common Linux and MacOS systems. Whenever available, wrapped software is built from versioned releases creating a frozen set with the respective pipeline release.

The internal wrappers are used to automatically build and deploy Singularity images for preset modules and pipeline versions. The images are automatically pulled once a module is used. This mechanism enables the complete function set of \textit{Snakemake} and \textit{Nanopype} while only requiring a system-wide Python and Singularity installation. In contrast to Docker, the execution of Singularity containers does not require root privileges on the target system at run time.
An all-in-one Singularity container is provided, wrapping the entire pipeline into a single environment. Primarily aimed for stand-alone usage, Windows systems, and for initial testing, this method does not offer support for cluster computation.

While greatly simplifying the installation process, containerized software comes at the cost of the container size. Whereas the \textit{minimap2} binary alone is 1 MB small, the size of the compressed \textit{Nanopype} alignment image is 191 MB (v1.0.1) due to integration of alignment tools, samtools, libraries and the Ubuntu kernel. Especially within distributed environments where each cluster job needs to fetch these images from a file server, this may lead to noticeable overhead depending on the jobs core function.


\subsection{Transparency}
\label{subsec:nanopype:transparency}

Common file formats in bioinformatics such as FASTQ or BigWig do not support the direct documentation of the applied workflow. An exception is the BAM alignment format, which can preserve the executed command in the @PG header line. Nonetheless, the source of the sequences remains obscure. Extensive and continuously updated documentation is required, especially when multiple parallel projects and workflows are involved.

Nanopype follows the Snakemake concept of output file driven computation: a single user command typically provokes transparent processing of any required intermediate result. Preexisting tools are integrated into consistent workflows and provide standard output formats to connect to workflows of established next-generation sequencing data analysis tools. The processing and subsequent output is intuitively organized in modules and underlying application directories (Fig. \ref{fig:nanopype:dir_tree}).

\begin{figure}[h]
	\centering
	\begin{minipage}{.7\linewidth}
	\dirtree{%
		.1 ./.
		.2 sequences/.
		.3 guppy/.
		.4 batches/.
		.4 Hues8.fast.fastq.gz.
		.2 alignments/.
		.3 ngmlr/.
		.4 guppy/.
		.5 batches/.
		.5 Hues8.fast.hg38.bam.
		.2 methylation/.
		.3 nanopolish/.
		.4 ngmlr/.
		.5 guppy/.
		.6 batches/.
		.6 Hues8.fast.10x.hg38.bw.
	}
	\end{minipage}
	\captionsetup{format=plain}
	\caption[Typical Nanopype output directory]{Typical \textit{Nanopype} output directory structure after basecalling, alignment and methylation detection. Top level directories represent pipeline modules with the applied tools in subjacent levels. \textbf{Hues8.fast} is the user defined tag of the output, in this case for the HUES8 cell line and guppy basecalling in fast mode. \textbf{10x} is the coverage wildcard, indicating to filter for CpGs with at least 10 reads coverage. \textbf{Hg38} defines the reference genome and \textbf{.bw} the output file format.}
	\label{fig:nanopype:dir_tree}
\end{figure}

The full reach of this concept becomes visible in the case of alternative workflows, re-processing and downstream analysis. Requesting for instance the output file \path{methylation/nanopolish/minimap2/guppy/Hues8.fast.10x.hg38.bw} would detect the presence of basecalled reads for the tag \textbf{Hues8.fast} and run only the subsequent alignment and methylation detection. While the processing of \path{sv/sniffles/ngmlr/guppy/Hues8.fast.hg38.vcf.gz} would utilize the already existing alignments, the same workflow but for reference \textbf{hg19} would automatically generate the missing intermediate alignments.
Lastly, any analysis using \textit{Nanopype} as a pre-processing step can rely on filename patterns and the fixed output directory structure.




\section{Modules}
\label{sec:nanopype:modules}
Nanopype’s backbone consists of modules that resolve a specific task, like basecalling, alignment or further downstream analyses. If available, alternative applications are provided for the same task and grouped into a module with a coherent output format. Integrating first and foremost low-level nanopore data processing applications provided by ONT, established community developed software packages have been included in the first Nanopype release as well.


\subsection{Basecalling}
\label{subsec:nanopype:basecalling}
The basecalling module translates raw nanopore signals into nucleotide sequences and is utilized by most subsequent pipeline layers. With the initial release, we include the established packages Guppy, Albacore, and Flappie, all provided by ONT \cite{Wick2019}. The default basecaller package is set to the recently released Guppy. Albacore is supported for backward compatibility but deprecated by ONT. The experimental Flappie is ONT’s first DNA methylation-aware basecaller. It extends the usual four-letter nucleotide alphabet by a fifth letter for methylated cytosine in CpG contexts. For all basecallers, the output is the standardized FASTQ and supplemented by us with a basic quality control summary.


\subsection{Alignment}
\label{subsec:nanopype:alignment}
The core functionality of the pipeline is the alignment of reads against a reference genome or draft assembly. Here, we provide three different aligners with distinct advantages, which make them favorable for different applications downstream. While Minimap2 \cite{Li2018} is a fast, low memory footprint solution suitable for both DNA and RNA alignments, GraphMap \cite{Sovic2016} is a sensitive aligner but with comparably high memory requirements. NGMLR \cite{Sedlazeck2018} is the recommended tool for the structural variation module. Any combination of basecalling, alignment, and reference genome is supported and reports BAM format files.


\subsection{DNA methylation}
\label{subsec:nanopype:methylation}
Sequencing without prior DNA amplification enables the direct readout of DNA base modifications. The current state of the art approach, Nanopolish \cite{Simpson2017} and the more experimental flip-flop basecaller Flappie, are incorporated into Nanopype. Subsequently, Nanopype splits Flappie’s atypical sequence output into standard FASTQ and methylation status. DNA methylation at CpG dinucleotides of both tools is reported in a table format for single reads. Furthermore, we provide standard bedGraph and BigWig files for genome-wide methylation tracks and thus enable downstream processing and visualization using established workflows, e.g., calling of differential methylated regions and comparison to bisulfite-sequencing.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/nanopype/sr_methylation.pdf}
	\captionsetup{format=plain}
	\caption[Nanopype single read methylation track]{Nanopype single-read methylation track of nanopore sequenced HUES8. Mean methylation track of CpGs with >=10\textit{x} coverage. Single-read methylation computed from original alignment by substituting unmethylated Cs to Ts (imitated bisulfite conversion, blue). Unclassified sites (abs. nanopolish log-likelihood-ratio < 2.0) are substituted with As and appear as mismatches (orange). All remaining mismatches in the nanopore reads are replaced with the genomic sequence. Shown are reads overlapping DIRAS3, an imprinted gene on genome build hg38, grouped by allele and shaded by strand.}
	\label{fig:nanopype:sr_methylation}
\end{figure}


\subsection{Structural variation}
\label{subsec:nanopype:sv}
Detection and characterization of structural variation play a central role in cancer research and population genetics. Long read sequencing particularly facilitates investigation of variants with unprecedented accuracy and resolution. Therefore, Nanopype encompasses the variant caller Sniffles \cite{Sedlazeck2018} and provides output in the standard variant calling format (VCF).


\subsection{Transcriptome}
\label{subsec:nanopype:transcriptom}
Another application of the long read nanopore technology is sequencing of cDNA and RNA molecules directly. For instance, recovery of full-length transcripts enables, the detection of alternatively spliced isoforms and is implemented in Nanopype using the Pinfish package. The output of polished transcripts is provided in the GFF format.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/nanopype/rna_isoforms.pdf}
	\captionsetup{format=plain}
	\caption[Nanopore direct RNA sequencing]{Nanopore direct RNA sequencing of H1 from one MinION flow cell. Shown are \textit{minimap2} splice-aligned reads spanning DNMT3B, obtained by using the RNA sequencing without amplification. The isoform detected from the \textit{Pinfish} package matches one of the reference isoforms.}
	\label{fig:nanopype:rna_isoforms}
\end{figure}


\subsection{Genome Assembly}
\label{subsec:nanopype:assembly}

\textit{De novo} assembly of genomes requires only the long read sequences and benefits barely from the integration into a pipeline. Due to its own cluster back end, the still widely used Canu \cite{Koren2017} has not been included. The recently published Flye \cite{Kolmogorov2019} and wtdbg2 \cite{Ruan2020} assemblers provide comparable continuity of the output genome while reducing the required compute time. Both are part of Nanopype, enabling basic assemblies already from 20-30x coverage.

Complemented by SAMtools \cite{Li2009}, BEDtools \cite{Quinlan2010} and UCSCtools \cite{Kent2010} our pipeline establishes a comprehensive framework for ONT sequencing data processing.




\section{Installation}
\label{sec:nanopype:installation}

The installation process of dependencies is a crucial, though sometimes neglected step to deploy reproducible workflows. A commonly found bypass is to require tools to be discovered through the user's PATH variable, with a remaining uncertainty of particular versions and thus limiting the outcome to the order but not necessarily exact behavior of applied tools. We address this issue by providing installation wrappers, enforcing tool versions to be bound to each individual pipeline version. Programs and files are identified by their absolute path, making the approach robust against changes of environment variables between local and cluster servers. Within \textit{Nanopype}, consistent versioning of source and container builds ensures reproducibility independent of the installation method.


\subsection{Source}

We utilize the workflow engine Snakemake to automate cloning and building of open source software. The build from source is the most customizable setup option and moreover allows the integration into complex, preexisting environments. The snakefile \textit{rules/install.smk} in the \textit{Nanopype} repository contains installation wrappers for all included tools, except proprietary software like the basecaller albacore. For example, the following command would clone and build the minimap2 binary required by the currently installed pipeline version. A more detailed description of the Snakemake command line is given in section \ref{sec:nanopype:usage}.

\begin{lstlisting}[language=sh, caption=Snakemake tool installation example]
snakemake --snakefile ~/src/nanopype/rules/install.smk --directory ~/ minimap2
\end{lstlisting}

After completion, the home directory will contain the common \textit{src}, \textit{lib} and \textit{bin} folders and the executable $\sim$\textit{/bin/minimap2}. As an example, the source of the minimap2 build rule is given in listing \ref{lst:nanopype:build}.

A special case when building from source is formed by heterogeneous server cluster environments in combination with vector instructions such as MMX, SSE and AVX. Depending on the software, the support of these parallel instructions is determined at build- and not at run time. Compiling the toolset of \textit{Nanopype} on a modern CPU with e.g. AVX2 can cause program crashes (Signal 4, illegal instruction) on other older nodes. A simple solution can be building the pipeline on a node with the largest common subset of advanced CPU instructions.


\subsection{Container}

Docker and Singularity enable the encapsulation of tools and dependencies into virtualized software containers. Packaging libraries and data, these containers can be executed on any operating system with just the virtualization set up. While the source installation method of \textit{Nanopype} aims to minimize dependencies, a functional C/C++ tool chain and a basic set of development libraries are required to build all parts of the pipeline. To further reduce both, target site dependencies and installation time, we provide pre-build Singularity containers per pipeline module and as all-in-one.

\textit{Nanopype} module images are build and tested on Travis-CI and deployed as Docker containers to \textit{https://hub.docker.com/u/nanopype/}. Being executed by \textit{Singularity} in the \textit{Snakemake} backend, both the Docker and Singularity container formats are supported, allowing a choice based on personal preferences.
Webhooks from Github trigger builds on each push to master and development branch. Tagged commits (e.g. \textit{v1.0.1}) to the master branch are build as tagged images. Upon execution the pipeline version is inferred from the git tag and associated images are pulled as needed for the workflow.


In order to minimize the size of each container, all \textit{Nanopype} modules rely on a staged build process. A \textit{base} and a \textit{build} container are set up in a first step, one with the pipeline and its basic dependencies, the other with build tools and libraries. Within each module's Dockerfile two separate stages are configured. The first build stage inherits from the \textit{build} container. Bioinformatic tools required by \textit{Nanopype} are compiled and linked in this stage. As an example, the build process for the alignment module is shown in listing \ref{lst:nanopype:docker}.

\lstinputlisting[language=docker, caption=Staged Docker build, label=lst:nanopype:docker]{listings/nanopype/docker_staged.txt}

A subsequent packaging stage inherits from the \textit{base} container and copies all needed binaries from the build stage and requires only the installation of runtime libraries for dynamically linked executables. The build stage is dropped in the final compressed image. 

The all-in-one container works in a similar way, by first pulling all module containers into intermediate layers, copying their binaries and squashing everything into a single image. The cascaded build process from source over module to all-in-one container reduces redundant code to a minimum and enhances the maintainability of the pipeline. Thus, to update or add additional tools, only the source build rules need to be edited.


\subsection{Configuration}

The last part of the installation is the site dependent configuration. Nanopype has two configuration layers: The central environment configuration \textit{env.yaml} covers application paths and reference genomes and is set up independent of installation method and operating system once. The environment configuration is stored in the installation directory. For each project an additional workflow configuration is required, providing data sources, tool flags and parameters. The workflow config file \textit{nanopype.yaml} is expected to be found in the root of each processing directory. Configuration files are in .yaml format; examples with default values can be found in the pipeline repository.

If a compute cluster is available, the respective Snakemake configuration is only needed once per Nanopype installation. Already available presets support a custom scheduler called mxq and the common SLURM scheduler. Presets are stored in the profiles folder of the repository and can be extended by pull requests through the community. A compute rule can, depending on the number of threads assigned, define memory and time requirements. These parameters can be forwarded to the cluster scheduler upon execution. Both memory and run time are conservatively pre-configured but can be adjusted via individual offset and scaling parameters in the environment configuration. 

Nanopype makes extensive use of the Snakemake shadow directory mechanism. Per default, rules are executed directly in the working directory. By specifying a shadow-directory, the input of supporting rules is linked into a temporary location and only expected outputs are copied to the working directory. Intended for the isolation of tools producing temporary intermediate outputs, this concept can also be used to move I/O heavy computations to local hard drives on each compute node and thus reducing the load on network file systems. Given that each compute node in the cluster has a local disk mounted as e.g. \textit{/scratch/local}, the Snakemake shadow-prefix would be set to this path.




\section{Usage}
\label{sec:nanopype:usage}
Due to the functional range of Nanopype, dependent on the operating system and selected installation method the setup can require advanced system administrative knowledge. However, after deployment, the subsequent usage is straightforward, given basic command line understanding. Complete Nanopype workflows can be executed with a single concise command line call. For instance, local processing of multiple flow cells into a collective genome-wide methylation track of at least 5x coverage on reference hg38 requires only the following call:

\begin{lstlisting}[language=sh, caption=Snakemake example]
snakemake --snakefile ~/nanopype/Snakefile methylation/nanopolish/ngmlr/guppy/Hues8.5x.hg38.bw
\end{lstlisting}

This command invokes basecalling, alignment and methylation detection using declared tools without further user interaction. The basecalling and alignment outputs are kept and can be reused to avoid redundant processing.

The output path encodes the applied toolchain, the filename is composed of the tag and a sequence of workflow dependent wildcards. For the given example, \textbf{Hues8} is the name of the cell line, while \textbf{5x} and \textbf{hg38} are parsed by the pipeline to determine minimum coverage and reference genome. Different tags can be used to test different settings on the same data set e.g. basecalling with guppy in fast and high-accuracy mode could be indicated by \textbf{Hues8.fast} and \textbf{Hues8.hac}.

Tags are global, changing only the alignment settings and re-running a workflow would still trigger a new basecalling, since sequences for the new tag are not available. 


\subsection{Batch processing}
\label{subsec:nanopype:batchprocessing}
Nanopype exploits the storage of raw nanopore data into batches of single reads. The automatic distribution of workflows into independent compute portions enables efficient handling of high-throughput experiments. This feature becomes particularly relevant for scaling in cluster environments and, most importantly in case of terminated or failed jobs. As a result, only failed batches require reprocessing by resuming the workflow from where it left off, using the same command, which enhances the overall error robustness. 
Nanopype keeps the dataset separated as long as possible: For instance does the methylation detection of a single read only require its sequence and alignment, while the structural variant detection requires all alignments to be merged into a single sorted file.

When working with batches of nanopore reads, two limitations of the underlying operating system require special handling: The maximum command line length and the limit of parallel opened files. Taking the example of sequence alignment and sorting, a naive merge of all batches into a single BAM file would call \textit{samtools merge} followed by the list of independently computed alignment batches. For high throughput experiments, this is likely to break either the maximum command argument length (getconf ARG\_MAX, on ubuntu e.g. 2097152) or the maximum allowed open files per process (ulimit -Sn, on ubuntu e.g. 1024). The former is solved by reading from files of filenames (.fofn) and piping data through stdin, the latter by first merging multiple times with the maximum opened files and a final merge of these intermediate results.


\subsection{Barcoding}
\label{subsec:nanopype:barcoding}
Barcoded sequencing allows pooling of multiple samples on a single flow-cell. Thus, sequencing of comparable small bacterial genomes can be efficiently parallelized to use the available sequencing depth optimally. The corresponding demultiplexing is a special transparent module in Nanopype. Using Deepbinner \cite{Wick2018} on signal or guppy on sequence level, it assigns a barcode label to each individual read.  The following example illustrates the command to only process reads of barcode NB01 from the ONT native barcoding kit:

\begin{lstlisting}[language=sh, caption=Nanopype demultiplexing]
snakemake --snakefile ~/nanopype/Snakefile sequences/guppy/Ecoli.NB01.fastq.gz
\end{lstlisting}

The pipeline automatically scans the previously introduced tag for substrings indicating the usage of a barcode. Barcodes are specified through an additional config file \textit{barcodes.yaml}.
Indexing the content of both packaged and multi-FAST5 output enables the fast retrieval of individual reads by their ID. The demultiplexing module first generates batches of read IDs per barcode and then temporarily extracts those reads for downstream processing.


\subsection{Logging and Reports}
\label{subsec:nanopype:logging}

Finally, an extensive logging of both, job specific output on each compute node and a summary of configuration values, ensures the complete documentation of the workflow. The quality control of the sequencing is enabled by automatic pdf reports containing basic statistics as read lengths, mapping rates and coverage. The report is obtained by the following command and contains dynamic sections for each of the pipeline modules.

\begin{lstlisting}[language=sh, caption=Nanopype report]
snakemake --snakefile ~/nanopype/Snakefile report.pdf
\end{lstlisting}

An example report is attached in supplement \ref{sec:supplement:nanopype:report}.




\section{Summary}
\label{sec:nanopype:summary}
Nanopype is a modular and easy-to-use data processing pipeline with a detailed online documentation, specifically designed to handle nanopore sequenced long read data.
Nanopype provides end-to-end processing of the raw sequencer signal into standard data formats and consequently closes the gap to downstream next-generation sequencing algorithms. Single command invocations of entire workflows reduce the hands-on-time for users to receive the desired output. Implicitly, this also lowers the potential of user mistakes and deviations in processing of multiple data sets. Consequently, workflows are easier to reproduce with fixed versions among datasets or repeated with improved tool releases on existing ones.
Nanopype is implemented as a Python package and additionally provides pre-built and versioned Singularity and Docker images, making it favorable for effective usage in cluster and single computer environments. The pipeline design ensures portability and version controlled usage of the implemented tools, to enable consistent results across platforms and laboratories.


